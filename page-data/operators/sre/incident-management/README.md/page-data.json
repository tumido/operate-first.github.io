{"componentChunkName":"component---src-templates-markdown-js","path":"/operators/sre/incident-management/README.md","result":{"data":{"site":{"siteMetadata":{"title":"Operate First"}},"markdownRemark":{"id":"ded31c1a-9e19-536c-857e-48c36ab5c45f","html":"<h2 id=\"incident-management\" style=\"position:relative;\"><a href=\"#incident-management\" aria-label=\"incident management permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Incident Management</h2>\n<p>This folder contains information pertaining to how the Operate First team will respond to incidents/outages.</p>\n<p>As we have multiple services/applications deployed and monitored in the Operate First environment (ex. Jupyterhub, Argo, Superset, Observatorium, Project Thoth, AICoE CI pipelines etc), we need to implement an incident reporting setup for handling outages/incidents related to these services. Whether its a major bug, capacity issues, or an outage, our users expect an immediate response. Having an efficient incident management process is critical in ensuring that the incidents are always communicated to the users (via on-call notification systems) and handled by the team immediately. An on-call notification system is a system/software that provides an automated means of contacting users and communicating pertinent information during an incident. It also has additional on-call scheduling features that can be used to ensure that the right people on the team are available to address a problem during an incident.</p>\n<h3 id=\"github-alertmanager-receiver\" style=\"position:relative;\"><a href=\"#github-alertmanager-receiver\" aria-label=\"github alertmanager receiver permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>GitHub Alertmanager Receiver</h3>\n<p>We have chosen the <a href=\"https://github.com/m-lab/alertmanager-github-receiver\">GitHub Alertmanager</a> as our tool for reporting the outages/incidents to our users. The GitHub alertmanager is a Prometheus alertmanager webhook receiver that creates GitHub issues from alerts. It can easily be configured and operated to function with Prometheus alerts. All communication/updates/concerns related to the incident can be easily handled by adding comments in the issues created by the GitHub receiver, making the incident management process completely transparent to our users. We can also configure GitHub bots for different chatting platforms such as Slack, Google Chat to notify us of the issues immediately.</p>\n<p>To deploy and setup your own GitHub alertmanager receiver, follow the instructions <a href=\"https://github.com/operate-first/SRE/blob/master/incident-management/github-receiver-setup.md\">here</a>.</p>\n<h3 id=\"alerting-with-prometheus\" style=\"position:relative;\"><a href=\"#alerting-with-prometheus\" aria-label=\"alerting with prometheus permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Alerting with Prometheus</h3>\n<p>All the services are being monitored by Prometheus. Prometheus scrapes and stores time series data identified by metric key/value pairs for each of the available services. We use these metrics to define the <a href=\"https://github.com/operate-first/SRE/tree/master/sli-slo\">SLI/SLOs</a> for each of our services and alert on any possible service degradation.</p>\n<p>Alerting with Prometheus is separated into two parts:</p>\n<ol class=\"pf-c-list\">\n<li><a href=\"https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/\">Alerting rules</a>  in Prometheus servers send alerts to an <a href=\"https://prometheus.io/docs/alerting/latest/alertmanager/\">Alertmanager</a>. Alerting rules allow you to define alert conditions based on Prometheus expression language expressions.</li>\n<li>The Alertmanager then manages those alerts, including silencing, inhibition, aggregation and sending out notifications via methods such as email, on-call notification systems, and chat platforms.</li>\n</ol>\n<p>We have added our GitHub alertmanager receiver as the default receiver for our Prometheus alertmanager to route the alerts. For any alert that is being fired, the GitHub receiver will automatically create an issue in this repository like so: <a href=\"https://github.com/operate-first/SRE/issues/34\">https://github.com/operate-first/SRE/issues/34</a>.</p>\n<p>You can refer to the documentation <a href=\"https://github.com/operate-first/SRE/blob/master/incident-management/configure-prometheus-alerts.md\">here</a> which describes how these alerting rules are setup for our Operate First monitoring stack.</p>","fields":{"srcLink":"https://github.com/operate-first/SRE/blob/master/incident-management/README.md"},"frontmatter":{"title":"","description":null}}},"pageContext":{"id":"ded31c1a-9e19-536c-857e-48c36ab5c45f"}},"staticQueryHashes":["117426894","3000541721"]}