{"componentChunkName":"component---src-templates-markdown-js","path":"/data-science/ceph-drive-failure/docs/blog/hard-drive-failure-prediction-blog.md","result":{"data":{"site":{"siteMetadata":{"title":"Operate First"}},"markdownRemark":{"id":"afaac051-95a5-5076-89b8-1f91e5a39484","html":"<p><em>Authors:</em>  Karan Chauhan (kachau@redhat.com), Michael Clifford (mcliffor@redhat.com)</p>\n<p><em>Date Created:</em> 11 December 2020</p>\n<p><em>Date Updated:</em> 17 December 2020</p>\n<p><em>Tags:</em> supervised learning, time series, classification, ceph</p>\n<h2 id=\"abstract\" style=\"position:relative;\"><a href=\"#abstract\" aria-label=\"abstract permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Abstract</h2>\n<p>Many large-scale distributed storage systems, such as Ceph, use mirroring or erasure-coded redundancy to provide fault tolerance. Because of this, scaling storage up can potentially be resource-intensive. In this project we aim to mitigate this issue using machine learning. Our goal is to build an ML model to predict if a hard drive will fail within a predefined future time interval. These predictions can then be used by Ceph (or other similar systems) to create or destroy replicas accordingly, thus making it more resource efficient. The open source models we built outperformed the ProphetStor model currently used as default in Ceph. Additionally, we framed the problem in a Kaggle kernel format to catalyze community participation in this task.</p>\n<h2 id=\"introduction\" style=\"position:relative;\"><a href=\"#introduction\" aria-label=\"introduction permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Introduction</h2>\n<p>More than a million terabytes of data gets generated every day, and every bit of it is valuable. That’s why modern storage systems need to be reliable, scalable, and efficient. To provide fault tolerance and system availability, Ceph uses policy based data replication. It tries to store replicas on hard disks whose failure is likely not correlated, based on heuristics like physical proximity, shared power sources, and shared networks[1].</p>\n<p>However, we want to go beyond heuristics for determining failure. The probability of data loss is proportional to the probability of multiple concurrent device failures. So if we could accurately anticipate failure and add additional redundancy in the system before it occurs, we could increase data durability by up to an order of magnitude. Thus, our goal in this project is to build machine learning models to predict when a hard drive would fail.</p>\n<h2 id=\"data\" style=\"position:relative;\"><a href=\"#data\" aria-label=\"data permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Data</h2>\n<p>Using the right data is critical for creating meaningful models. With that in mind, we used SMART metrics collected from hard drives as our training data. SMART is a monitoring system that reports various indicators of drive reliability such as read error rate, uncorrectable sector count, etc[2]. We used the SMART metrics dataset that Backblaze has curated and made publicly available[3]. In addition to SMART metrics, this dataset also contains labels that specify whether or not a drive failed on a given date. Having these labels is incredibly powerful as it opens the door for training supervised machine learning models.</p>\n<table class=\"pf-c-table\">\n<thead>\n<tr>\n<th>date</th>\n<th>serial no.</th>\n<th>smart 1 raw</th>\n<th>smart 1 normalized</th>\n<th>…</th>\n<th>smart 250 normalized</th>\n<th>failure</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>2020-01-01</td>\n<td>hdd1234</td>\n<td>10</td>\n<td>100</td>\n<td>…</td>\n<td>100</td>\n<td>0</td>\n</tr>\n<tr>\n<td>2020-01-01</td>\n<td>hdd5678</td>\n<td>12</td>\n<td>99</td>\n<td>…</td>\n<td>100</td>\n<td>0</td>\n</tr>\n<tr>\n<td>2020-01-02</td>\n<td>hdd1234</td>\n<td>23</td>\n<td>70</td>\n<td>…</td>\n<td>80</td>\n<td>1</td>\n</tr>\n<tr>\n<td>2020-01-02</td>\n<td>hdd5678</td>\n<td>13</td>\n<td>100</td>\n<td>…</td>\n<td>99</td>\n<td>0</td>\n</tr>\n</tbody>\n</table>\n<p>Table 1: Illustrative rendition of the Backblaze dataset</p>\n<h2 id=\"methodology\" style=\"position:relative;\"><a href=\"#methodology\" aria-label=\"methodology permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Methodology</h2>\n<p>Previous work done in this domain seemed to suggest that SMART metric behavior may vary across drive vendors. Also, only a handful of vendors collectively make up the majority of drives being used. For these two reasons, we decided to train one prediction model per vendor.</p>\n<p>The first step in the training pipeline was to clean the data. We will omit explanations for the data cleaning methods we used, but curious readers can check it out here[4]. Next, to determine which metrics to use as input features, we looked at the correlations of each metric with failure, and also referred to previous research done.</p>\n<p>Then, to determine what kind of models to build, we focused our attention to the fact that these models are intended to be first-class citizens in Ceph. So we needed to be mindful of any constraints involved with upstream integration. Ceph already had a health prediction module[5] that consumed 6-12 days of SMART metrics and used ProphetStor’s sample model[6] to estimate drive health as “bad” (fail in 2 weeks), “warning” (fail in 2-6 weeks), or “good” (fail in >6 weeks). With this, our requirements boiled down to the following</p>\n<ol class=\"pf-c-list\">\n<li>Model output should be “good”, “warning”, or “bad”</li>\n<li>At most 6 days of data can be <em>guaranteed</em> to be available at run time</li>\n</ol>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/03b33db0015a6b348ae67c629600cae3/0a47e/sampleinput.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 62.16216216216216%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAAAsTAAALEwEAmpwYAAABQUlEQVQoz42TC2/DIAyE+///56RszZskPIK5cVAnqRpps2RxBevjjNMHcuz7Duc9akjOVDMliKS8/D8fRPgM895icQn9Igh7wvcksL7CCb3mn8AYY3YZ4DPI2AQbBEMGE746dXw6F5HbLMCu62CthXP+1abUBAsEixV8dbGszN7EfFzPyiWXtQCfzye2bSttp/R5O4sJolOT158pwmyCdo5oxgifuxnX83kOhyGE2zfhmx1DeiX395gwZbAL9Wk2BU7TlNt1bw6v6+lah3Lun5dI+SKOoVDQoera6rtWkGoo6lJbgCrocJ5nrOta4MMwvIbliuY5z8ZxLN+tMQbsjpp7/E34Q2/t+x5t25ZNDooQfk5N02BZlgKk5gUEUxPGOtazlnHbsmrCVdMNXTB0gKo1jpavG3da/57Ma2jNOSTgF41CsN0LiUf4AAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"SampleInput\"\n        title=\"SampleInput\"\n        src=\"/static/03b33db0015a6b348ae67c629600cae3/fcda8/sampleinput.png\"\n        srcset=\"/static/03b33db0015a6b348ae67c629600cae3/12f09/sampleinput.png 148w,\n/static/03b33db0015a6b348ae67c629600cae3/e4a3f/sampleinput.png 295w,\n/static/03b33db0015a6b348ae67c629600cae3/fcda8/sampleinput.png 590w,\n/static/03b33db0015a6b348ae67c629600cae3/0a47e/sampleinput.png 600w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p>Figure 1: Sample model input (only one SMART metric shown)</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 65px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/513ea8cd4c073d9c763032e9c7d4171d/4247e/sampleoutput.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 127.69230769230771%; position: relative; bottom: 0; left: 0; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"SampleOutput\"\n        title=\"SampleOutput\"\n        src=\"/static/513ea8cd4c073d9c763032e9c7d4171d/4247e/sampleoutput.png\"\n        srcset=\"/static/513ea8cd4c073d9c763032e9c7d4171d/4247e/sampleoutput.png 65w\"\n        sizes=\"(max-width: 65px) 100vw, 65px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p>Figure 2: Sample model output</p>\n<p>Considering all these facts, we decided that building multiclass time series classification models was the way to go. Our approach for training models can be summarized as follows: for each 6-day sliding window, create a feature vector characterizing the multidimensional time series, and then feed this feature vector to a classification model. The label for each sliding window would be good/warning/bad based on how many days away from failure the drive was on the 6th (i.e. most recent) day.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/d3387ad991df2929a66796af368f7f96/844cc/featurization.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 46.621621621621614%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAAsTAAALEwEAmpwYAAABvElEQVQoz12QXVPTQBSG8w8UFERq09iY0KRpm2w2n5vPpkWGO8cbrxgUGOzgrb/AC2gZfvPr2RWrcPHM+dg97553tTAMIWGMqcg5V6RpiizLVHxCliJJUhSVQBRz+DMfQRBs0aTQc7Eoimgo+SdIIqIQEEIgiRNUbYF6USLkDL7/R/Bv1OSQRF6W5HmOpmlQ1zXKokRV1ejmHdqmRdM26E5aFHVOC/CtSOAHW1Gtqqqt2P+08wbHp0t8JJYnHeULLE7nyESqHMRx/GSzgKISlNsk8gJZlSTSMtnP8xR1K1BWGcpaukgQ0iaM4PRF/FGEPUOrSXBMr7kkqqC/c6l2eERQn/5sRPZUTX2bhGwSlEwod+hx67EesRDakiz/Go1wT2wcB+vhEBvbxsZ1sbEsrI0B7qwPuKXera7j7uhI5Q+2hU+ehxvTxD3NrCn+NAxoHQmeve3h+zsdV70+vu6+wsX+AVaD9/j2eh/nL3aworOVbuD85S4uDg5x3R/gcu8Njt0xPtPMD+KGel9oTivKEro3hck4jFmA3niKvjeDHSUql5gsUqjanWAYhDh0PHhk0Zz6sOMMTiZgTGb4DdP4Jf6lC2ygAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"SampleFeatures\"\n        title=\"SampleFeatures\"\n        src=\"/static/d3387ad991df2929a66796af368f7f96/fcda8/featurization.png\"\n        srcset=\"/static/d3387ad991df2929a66796af368f7f96/12f09/featurization.png 148w,\n/static/d3387ad991df2929a66796af368f7f96/e4a3f/featurization.png 295w,\n/static/d3387ad991df2929a66796af368f7f96/fcda8/featurization.png 590w,\n/static/d3387ad991df2929a66796af368f7f96/efc66/featurization.png 885w,\n/static/d3387ad991df2929a66796af368f7f96/c83ae/featurization.png 1180w,\n/static/d3387ad991df2929a66796af368f7f96/844cc/featurization.png 1306w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p>Figure 3: Creating feature vectors from each sliding window</p>\n<h2 id=\"results\" style=\"position:relative;\"><a href=\"#results\" aria-label=\"results permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Results</h2>\n<p>To evaluate models, we first created train and test sets. We ensured that data in these two sets was from completely different drives, so that there is no “leakage” across the two sets. We trained and evaluated a few classical models such as decision trees, GBDTs, random forest, etc[7]. Note that to ensure ease in downstream packaging, we limited ourselves to use only the python packages available in EPEL. Under this constraint, we found that a random forest classifier performed the best, and outperformed the SVM based sample model currently used as default on Ceph. Specifically, the results were as follows</p>\n<table class=\"pf-c-table\">\n<thead>\n<tr>\n<th>Model</th>\n<th>Good</th>\n<th>Warning</th>\n<th>Bad</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>ProphetStor</td>\n<td><strong>0.998</strong></td>\n<td>0.000</td>\n<td><strong>0.003</strong></td>\n</tr>\n<tr>\n<td>Red Hat</td>\n<td>0.971</td>\n<td><strong>0.001</strong></td>\n<td>0.002</td>\n</tr>\n</tbody>\n</table>\n<p>Table 2a: Recall score on each class for two models</p>\n<table class=\"pf-c-table\">\n<thead>\n<tr>\n<th>Model</th>\n<th>Good</th>\n<th>Warning</th>\n<th>Bad</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>ProphetStor</td>\n<td>0.489</td>\n<td>0.000</td>\n<td>0.266</td>\n</tr>\n<tr>\n<td>Red Hat</td>\n<td><strong>0.489</strong></td>\n<td><strong>0.310</strong></td>\n<td><strong>0.333</strong></td>\n</tr>\n</tbody>\n</table>\n<p>Table 2b: Precision score on each class for two models</p>\n<h2 id=\"conclusion\" style=\"position:relative;\"><a href=\"#conclusion\" aria-label=\"conclusion permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Conclusion</h2>\n<p>In this blog we discussed an important practical use case of data science in the operations domain. Specifically we described how we used machine learning to predict hard drive failure for Ceph, to make it more efficient. As seen in Fig 3, the approach we used seems to outperform the sample ProphetStor model. Still, we believe there is a lot of room for improvement. This work is intended to serve a catalyst for community involvement. To drive that home, we have made the code available[8,9] and also created a short “getting started”[10] jupyter notebook similar to a Kaggle kernel. Please feel free to explore this problem in detail, and open issues or submit PRs on our repo. If you’re a Ceph user, you can also contribute in improving the models by providing real usage data from your Ceph clusters via telemetry[11].</p>\n<h2 id=\"references\" style=\"position:relative;\"><a href=\"#references\" aria-label=\"references permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>References</h2>\n<p>[1] <a href=\"https://access.redhat.com/documentation/en-us/red_hat_amq/7.5/html/configuring_amq_broker/configuring-fault-tolerant-system-configuring#about-ceph-storage-clusters-configuring\">How Red Hat Ceph Storage clusters work</a></p>\n<p>[2] <a href=\"https://en.wikipedia.org/wiki/S.M.A.R.T.\">S.M.A.R.T</a></p>\n<p>[3] <a href=\"https://www.backblaze.com/b2/hard-drive-test-data.html\">Backblaze Dataset</a></p>\n<p>[4] <a href=\"https://github.com/aicoe-aiops/ceph_drive_failure/blob/master/notebooks/step2a_data_cleaner_seagate.ipynb\">Data Cleaning Jupyter Notebook</a></p>\n<p>[5] <a href=\"https://docs.ceph.com/en/latest/mgr/diskprediction/\">Ceph diskprediction module</a></p>\n<p>[6] <a href=\"https://github.com/ceph/ceph/tree/master/src/pybind/mgr/diskprediction_local/models/prophetstor\">ProphetStor model</a></p>\n<p>[7] <a href=\"https://github.com/chauhankaranraj/ceph_drive_failure/blob/master/notebooks/step3b_ternary_clf.ipynb\">Classifier Models Exploratory Jupyter Notebook</a></p>\n<p>[8] <a href=\"https://github.com/aicoe-aiops/ceph_drive_failure\">GitHub repo</a></p>\n<p>[9] <a href=\"https://github.com/AICoE/disk-failure-prediction\">GitHub repo with Kaggle-like setup</a></p>\n<p>[10] <a href=\"https://github.com/AICoE/disk-failure-prediction/blob/master/Getting_Started.ipynb\">Getting Started Jupyter Notebook</a></p>\n<p>[11] <a href=\"https://docs.ceph.com/en/latest/mgr/telemetry/\">Ceph Telemetry</a></p>\n<h3 id=\"project-material\" style=\"position:relative;\"><a href=\"#project-material\" aria-label=\"project material permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Project Material</h3>\n<h4 id=\"steps-to-get-you-started\" style=\"position:relative;\"><a href=\"#steps-to-get-you-started\" aria-label=\"steps to get you started permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Steps to get you started:</h4>\n<ol class=\"pf-c-list\">\n<li>Visit <a href=\"https://jupyterhub-opf-jupyterhub.apps.cnv.massopen.cloud/hub/login\">https://jupyterhub-opf-jupyterhub.apps.cnv.massopen.cloud/hub/login</a></li>\n<li>Login with your google account</li>\n<li>In this Spawn screen, select ceph-drive-failure:latest</li>\n<li>Once your server starts, go into the directory named ceph-drive-failure-yyyy-mm-dd-hh-mm</li>\n<li>Go to <code class=\"language-text\">notebooks</code>. You can either run the step-by-step notebooks one after the other, or run the end-to-end notebooks.</li>\n</ol>\n<h4 id=\"how-to-contribute--provide-feedback\" style=\"position:relative;\"><a href=\"#how-to-contribute--provide-feedback\" aria-label=\"how to contribute  provide feedback permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>How to Contribute / Provide feedback</h4>\n<ul class=\"pf-c-list\">\n<li>Github Repository: <a href=\"https://github.com/aicoe-aiops/ceph_drive_failure\">https://github.com/aicoe-aiops/ceph_drive_failure</a></li>\n<li>You can open up a PR on the Git Repository highlighting the feature or issue, and we will address it.</li>\n<li>You can also reach out to kachau@redhat.com for any questions.</li>\n</ul>","fields":{"srcLink":"https://github.com/aicoe-aiops/ceph_drive_failure/blob/master/docs/blog/hard-drive-failure-prediction-blog.md"},"frontmatter":{"title":"Hard Drive Failure Prediction in Ceph","description":"Machine learning models for predicting disk failure in Ceph clusters"}}},"pageContext":{"id":"afaac051-95a5-5076-89b8-1f91e5a39484"}},"staticQueryHashes":["117426894","3000541721"]}